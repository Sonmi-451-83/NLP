{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install wordcloud unidecode nltk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import requests\n",
                "from pickle import dump\n",
                "import os\n",
                "import zipfile\n",
                "import tensorflow as tf\n",
                "from pathlib import Path\n",
                "import shutil\n",
                "import re\n",
                "import nltk\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from unidecode import unidecode\n",
                "from sklearn.model_selection import train_test_split\n",
                "from wordcloud import WordCloud\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.svm import SVC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nltk.download('stopwords')\n",
                "nltk.download('punkt')\n",
                "nltk.download('punkt_tab')\n",
                "nltk.download('wordnet')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "url = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\"\n",
                "response = requests.get(url).content.decode('utf-8')\n",
                "\n",
                "file_name = '/content/gdrive/MyDrive/Colab Notebooks/Data/url_spam.csv'\n",
                "\n",
                "with open(file_name, 'w') as temp_file:\n",
                "    temp_file.writelines(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_name = '/content/gdrive/MyDrive/Colab Notebooks/Data/url_spam.csv'\n",
                "df = pd.read_csv(file_name)\n",
                "pd.set_option('display.max_columns', None)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_url(text):\n",
                "    # Texto a minúscula\n",
                "    text = text.lower()\n",
                "    # Texto sin el http/https/www al principio\n",
                "    text = re.sub(r\"http\\S|www\\S|https\\S+\", '', text)\n",
                "    # Texto sin el .com al final\n",
                "    text = re.sub(r'.com/', ' ', text)\n",
                "    # Solo texto\n",
                "    text = re.sub(r'[^\\w]', ' ', text)\n",
                "    # Eliminación de los espacios adicionales\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    # Tokenización del texto\n",
                "    tokens = word_tokenize(text)\n",
                "    # Eliminación de los acentos\n",
                "    tokens = [unidecode(token) for token in tokens]\n",
                "    # Lematización de palabras\n",
                "    lemmatizer = WordNetLemmatizer()\n",
                "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
                "    return ' '.join(tokens)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ejemplo de aplicación de la función de preprocesado para una url\n",
                "url_example = df.url[6]\n",
                "print(f'Tweet antes de ser preprocesado:\\n {url_example}')\n",
                "url_example_prepro = preprocess_url(url_example)\n",
                "print('-'*50)\n",
                "print(f'Tweet tras ser preprocesado:\\n {url_example_prepro}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocesar el dataset\n",
                "\n",
                "df['url_prepro'] = df.url.apply(preprocess_url)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.url_prepro.duplicated().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Eliminar las url que tras ser preprocesadas han quedado igual\n",
                "df.drop_duplicates(subset='url_prepro', inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.is_spam.value_counts(normalize=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "El dataset está claramente desbalanceado hacia urls que no son spam."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Creamos las nubes de palabras\n",
                "\n",
                "spam = df['is_spam'].unique()\n",
                "\n",
                "for sp in spam:\n",
                "    text = \" \".join(df[df['is_spam'] == sp]['url_prepro'].tolist())\n",
                "    wordcloud = WordCloud(background_color=\"white\", max_words=50, contour_color=\"steelblue\", collocations=True)\n",
                "    wordcloud.generate(text)\n",
                "    # Mostrar la nube de palabras para cada partido\n",
                "    plt.imshow(wordcloud, interpolation='bilinear')\n",
                "    plt.title(f\"Nube de palabras - spam: {sp}\")\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(df.url_prepro, df.is_spam, test_size=0.2, random_state=42)\n",
                "vectorizer = TfidfVectorizer()\n",
                "X_train_vec = vectorizer.fit_transform(X_train)\n",
                "X_test_vec = vectorizer.transform(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vectorizer.get_feature_names_out()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clf = LogisticRegression().fit(X_train_vec, y_train)\n",
                "y_pred = clf.predict(X_test_vec)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_train = clf.predict(X_train_vec)\n",
                "print(classification_report(y_train, y_pred_train))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_test= clf.predict(X_test_vec)\n",
                "print(classification_report(y_test, y_pred_test))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Podemos observar con el recall que siempre acierta cuando false, y falla mucho cuando true, debido al desbalanceo del dataset, por lo que pasamos a balancear el dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clf2 = LogisticRegression(class_weight=\"balanced\").fit(X_train_vec, y_train)\n",
                "y_pred = clf2.predict(X_test_vec)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_train2 = clf2.predict(X_train_vec)\n",
                "print(classification_report(y_train, y_pred_train2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred_test2= clf2.predict(X_test_vec)\n",
                "print(classification_report(y_test, y_pred_test2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Podemos observar la mejora con el balanceo de la regresión lineal, aunque sigue lejos de ser óptima."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SVC(kernel = \"rbf\", C = 1.0, gamma = 0.5, class_weight=\"balanced\")\n",
                "model.fit(X_train_vec, y_train)\n",
                "\n",
                "y_pred_svc = model.predict(X_test_vec)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(classification_report(y_test, y_pred_svc))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Podemos observar que las métricas mejoran algo con respecto a la regresión lineal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
                "              'gamma': [1, 0.5, 0.1, 0.01, 0.001, 0.0001],\n",
                "              'kernel': ['rbf']}\n",
                "\n",
                "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "grid.fit(X_train_vec, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(grid.best_params_)\n",
                "\n",
                "print(grid.best_estimator_)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SVC(kernel = \"rbf\", C = 10, gamma = 0.1, class_weight=\"balanced\")\n",
                "model.fit(X_train_vec, y_train)\n",
                "\n",
                "y_pred_svc = model.predict(X_test_vec)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(classification_report(y_test, y_pred_svc))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finalmente conseguimos de nuevo una ligera mejora en Recall, consiguiendo el modelo más óptimo hasta la fecha."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.13 64-bit ('3.8.13')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.13"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
